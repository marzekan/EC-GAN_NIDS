{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-24 11:40:42.677099: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\n",
      "2021-12-24 11:40:42.677144: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'np_config' from 'tensorflow.python.ops.numpy_ops' (/home/osboxes/.local/share/virtualenvs/EC-GAN_NIDS-LDpVHeKH/lib/python3.8/site-packages/tensorflow/python/ops/numpy_ops/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9664/1463825329.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# enable_eager_execution()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnp_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mnp_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_numpy_behavior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'np_config' from 'tensorflow.python.ops.numpy_ops' (/home/osboxes/.local/share/virtualenvs/EC-GAN_NIDS-LDpVHeKH/lib/python3.8/site-packages/tensorflow/python/ops/numpy_ops/__init__.py)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from functools import partial\n",
    "import itertools\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Dropout, Embedding, multiply, LeakyReLU, ReLU, Softmax\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from tensorflow.python.framework.ops import disable_eager_execution, enable_eager_execution\n",
    "disable_eager_execution()\n",
    "# enable_eager_execution()\n",
    "\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior()\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physical GPUs: 1\n",
      "Logical GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(\"Physical GPUs:\", len(gpus))\n",
    "    print(\"Logical GPUs:\", len(logical_gpus))\n",
    "\n",
    "except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load(\"data/preserve50/x_train.npy\")\n",
    "y_train = np.load(\"data/preserve50/y_train.npy\")\n",
    "x_test = np.load(\"data/preserve50/x_test.npy\")\n",
    "y_test = np.load(\"data/preserve50/y_test.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomWeightedAverage(tf.keras.layers.Layer):\n",
    "    \"\"\"Provides a (random) weighted average between real and generated image samples\"\"\"\n",
    "    \n",
    "    def __init__(self, batch_size):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def call(self, inputs, **kwargs):\n",
    "        alpha = tf.random.uniform((self.batch_size, 1))\n",
    "        return (alpha * inputs[0]) + ((1 - alpha) * inputs[1])\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0]\n",
    "\n",
    "    \n",
    "class ECGAN():\n",
    "    def __init__(self, \n",
    "                 x_train, \n",
    "                 y_train, \n",
    "                 num_classes: int, \n",
    "                 latent_dim: int, \n",
    "                 batch_size: int,\n",
    "                 n_critic: int,\n",
    "                 conf_thresh: float,\n",
    "                 adv_weight: float):\n",
    "        \"\"\"Implement EC-GAN with an WCGAN-GP and MLP.        \n",
    "        \n",
    "        Attributes\n",
    "        ---------\n",
    "        x_train : numpy.ndarray\n",
    "            Real data without labels used for training.\n",
    "            (Created with sklearn.model_selection.train_test_split\n",
    "        \n",
    "        y_train : numpy.ndarray\n",
    "            Real data labels.\n",
    "            \n",
    "        num_classes : int\n",
    "            Number of data classes. Number of unique elements in y_train.\n",
    "            \n",
    "        data_dim : int\n",
    "            Data dimension. Number of columns in x_train.\n",
    "            \n",
    "        latent_dim : int\n",
    "            Dimension of random noise vector (z), used for training\n",
    "            the generator.\n",
    "            \n",
    "        batch_size : int\n",
    "            Size of training batch in each epoch.\n",
    "        \n",
    "        n_critic : int\n",
    "            Number of times the critic (discriminator) will be trained\n",
    "            in each epoch.\n",
    "            \n",
    "        conf_thresh : float\n",
    "            Confidence threshold. EC-GAN parameter which decides how good\n",
    "            the generated sample needs to be, for it to be fed to the \n",
    "            classifier.\n",
    "        \n",
    "        adv_weight : float\n",
    "            Adverserial weight. EC-GAN parameter which represents the \n",
    "            importance fake data has on classifier training.\n",
    "            Value has been taken from the original paper.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.x_train = x_train.copy()\n",
    "        self.y_train = y_train.copy()\n",
    "        \n",
    "        # Store labels as one-hot vectors.\n",
    "        self.y_train_onehot = to_categorical(y_train)\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.data_dim = x_train.shape[1]\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # WCGAN-GP parameters. \n",
    "        self.n_critic = n_critic\n",
    "        \n",
    "        # EC-GAN parameters.\n",
    "        self.conf_thresh = conf_thresh\n",
    "        self.adv_weight = adv_weight\n",
    "        \n",
    "        # Log training progress.\n",
    "        self.losslog = []\n",
    "        self.class_acc_log = []\n",
    "        self.class_loss_log = []\n",
    "\n",
    "        # Adam optimizer for WCGAN-GP, suggested by original paper.\n",
    "        optimizer = Adam(learning_rate=0.0005, beta_1=0.05, beta_2=0.9)\n",
    "\n",
    "        # Categorical crossentropy loss function for the classifier.\n",
    "        self.cce_loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "        # Build the generator, critic and classifier\n",
    "        self.generator = self.build_generator()\n",
    "        self.critic = self.build_critic()\n",
    "        self.classifier = self.build_classifier()\n",
    "\n",
    "        \n",
    "        #-------------------------------\n",
    "        # Construct Computational Graph\n",
    "        #       for the Critic\n",
    "        #-------------------------------\n",
    "\n",
    "        # Freeze generator's layers while training critic.\n",
    "        self.generator.trainable = False\n",
    "\n",
    "        # Data input (real sample).\n",
    "        real_data = Input(shape=self.data_dim, name=\"Real_data\")\n",
    "        # Noise input (z).\n",
    "        noise = Input(shape=(self.latent_dim,), name=\"Noise\")\n",
    "        # Label input.\n",
    "        label = Input(shape=(1,), name=\"Label\")\n",
    "        \n",
    "        # Generate data based of noise (fake sample)\n",
    "        fake_data = self.generator([noise, label])\n",
    "        \n",
    "        # Critic (discriminator) determines validity of the real and fake images.\n",
    "        fake = self.critic([fake_data, label])\n",
    "        valid = self.critic([real_data, label])\n",
    "        \n",
    "        # Construct weighted average between real and fake images.\n",
    "        interpolated_data = RandomWeightedAverage(self.batch_size)([real_data, fake_data])\n",
    "        \n",
    "        # Determine validity of weighted sample.\n",
    "        validity_interpolated = self.critic([interpolated_data, label])\n",
    "        \n",
    "        \n",
    "        # Use Python partial to provide loss function with additional\n",
    "        # 'averaged_samples' argument.\n",
    "        partial_gp_loss = partial(self.gradient_penalty_loss,\n",
    "                          averaged_samples=interpolated_data)\n",
    "        # Keras requires function names.\n",
    "        partial_gp_loss.__name__ = 'gradient_penalty' \n",
    "        \n",
    "        self.critic_model = Model(\n",
    "            inputs=[real_data, label, noise],\n",
    "            outputs=[valid, fake, validity_interpolated]\n",
    "        )\n",
    "        \n",
    "        self.critic_model.compile(loss=[self.wasserstein_loss,\n",
    "                                        self.wasserstein_loss,\n",
    "                                        partial_gp_loss],\n",
    "                                  optimizer=optimizer,\n",
    "                                  loss_weights=[1, 1, 10])\n",
    " \n",
    "        #-------------------------------\n",
    "        # Construct Computational Graph\n",
    "        #         for Generator\n",
    "        #-------------------------------\n",
    "\n",
    "        # For the generator we freeze other's layers.\n",
    "        self.critic.trainable = False\n",
    "        self.generator.trainable = True\n",
    "\n",
    "        # Sampled noise for input to generator.\n",
    "        noise = Input(shape=(self.latent_dim,), name=\"Noise\")\n",
    "        \n",
    "        # Add label to input.\n",
    "        label = Input(shape=(1,), name=\"Label\")\n",
    "        \n",
    "        # Generate data based of noise.\n",
    "        fake_data = self.generator([noise, label])\n",
    "\n",
    "        # Discriminator determines validity.\n",
    "        valid = self.critic([fake_data, label])\n",
    "\n",
    "        # Defines generator model.\n",
    "        self.generator_model = Model([noise, label], valid)\n",
    "        \n",
    "        self.generator_model.compile(loss=self.wasserstein_loss, \n",
    "                                     optimizer=optimizer)\n",
    "\n",
    "        \n",
    "        \n",
    "        #-------------------------------\n",
    "        # Construct Computational Graph\n",
    "        #   for the Classifier (real)\n",
    "        #-------------------------------\n",
    "        \n",
    "        # Real data classifier training\n",
    "        \n",
    "        real_data = Input(shape=self.data_dim, name=\"Real_data\")\n",
    "        \n",
    "        real_predictions = self.classifier(real_data)\n",
    "        \n",
    "        self.real_classifier_model = Model(real_data, real_predictions)\n",
    "        \n",
    "        self.real_classifier_model.compile(loss=\"categorical_crossentropy\",\n",
    "                                           optimizer=\"adamax\",\n",
    "                                           metrics=[\"accuracy\"])\n",
    "        \n",
    "        #-------------------------------\n",
    "        # Construct Computational Graph\n",
    "        #   for the Classifier (fake)\n",
    "        #-------------------------------\n",
    "        \n",
    "        # Fake data classifier training\n",
    "        \n",
    "        noise = Input(shape=(self.latent_dim,), name=\"Noise\")\n",
    "        fake_labels = Input(shape=(1,), name=\"Label\")\n",
    "        \n",
    "        real_data = Input(shape=self.data_dim, name=\"Real_data\")\n",
    "        \n",
    "        fake_data = self.generator([noise, fake_labels])\n",
    "        \n",
    "        fake_predictions = self.classifier(fake_data)\n",
    "        \n",
    "        self.fake_classifier_model = Model([noise, fake_labels], fake_predictions)\n",
    "        \n",
    "        self.fake_classifier_model.compile(loss=self.ecgan_loss, \n",
    "                                           optimizer=\"adamax\",\n",
    "                                           metrics=[\"accuracy\"])\n",
    "\n",
    "        \n",
    "        \n",
    "    def ecgan_loss(self, y_true, y_pred):\n",
    "        \"\"\"Calculate loss for fake data predictions.\"\"\"\n",
    "        \n",
    "        max_values = tf.math.reduce_max(y_pred, axis=1)\n",
    "        \n",
    "        max_index = tf.where(tf.math.greater(max_values, self.conf_thresh))\n",
    "        \n",
    "        loss = self.adv_weight * self.cce_loss(y_true[max_index], y_pred[max_index])\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def gradient_penalty_loss(self, y_true, y_pred, averaged_samples):\n",
    "        \"\"\"\n",
    "        Computes gradient penalty based on prediction and weighted real / fake samples\n",
    "        \"\"\"\n",
    "        gradients = K.gradients(y_pred, averaged_samples)[0]\n",
    "        # compute the euclidean norm by squaring ...\n",
    "        gradients_sqr = K.square(gradients)\n",
    "        #   ... summing over the rows ...\n",
    "        gradients_sqr_sum = K.sum(gradients_sqr,\n",
    "                                  axis=np.arange(1, len(gradients_sqr.shape)))\n",
    "        #   ... and sqrt\n",
    "        gradient_l2_norm = K.sqrt(gradients_sqr_sum)\n",
    "        # compute lambda * (1 - ||grad||)^2 still for each single sample\n",
    "        gradient_penalty = K.square(1 - gradient_l2_norm)\n",
    "        # return the mean as loss over all the batch samples\n",
    "        return K.mean(gradient_penalty)\n",
    "\n",
    "\n",
    "    def wasserstein_loss(self, y_true, y_pred):\n",
    "        return K.mean(y_true * y_pred)\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential(name=\"Generator\")\n",
    "        \n",
    "        # First hidden layer.\n",
    "        model.add(Dense(256, input_dim=self.latent_dim))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.3))\n",
    "        \n",
    "        # Second hidden layer.\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.3))\n",
    "        \n",
    "        # Third hidden layer.\n",
    "        model.add(Dense(1024))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.3))\n",
    "        \n",
    "        # Output layer.\n",
    "        model.add(Dense(self.data_dim, activation=\"tanh\"))\n",
    "        \n",
    "        model.summary()\n",
    "        \n",
    "        # Noise and label input layers.\n",
    "        noise = Input(shape=(self.latent_dim,), name=\"Noise\")\n",
    "        label = Input(shape=(1,), dtype=\"int32\", name=\"Label\")\n",
    "        \n",
    "        # Embed labels into onehot encoded vectors.\n",
    "        label_embedding = Flatten(name=\"Flatten\")(Embedding(self.num_classes, self.latent_dim, name=\"Embedding\")(label))\n",
    "        \n",
    "        # Multiply noise and embedded labels to be used as model input.\n",
    "        model_input = multiply([noise, label_embedding], name=\"Multiply\")\n",
    "        \n",
    "        generated_data = model(model_input)\n",
    "\n",
    "        return Model(inputs=[noise, label], \n",
    "                     outputs=generated_data, \n",
    "                     name=\"Generator\")\n",
    "\n",
    "    def build_critic(self):\n",
    "\n",
    "        model = Sequential(name=\"Critic\")\n",
    "\n",
    "        # First hidden layer.\n",
    "        model.add(Dense(1024, input_dim=self.data_dim))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        \n",
    "        # Second hidden layer.        \n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        \n",
    "        # Third hidden layer.\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        # Output layer with linear activation.\n",
    "        model.add(Dense(1))\n",
    "\n",
    "        model.summary()\n",
    "        \n",
    "        # Artificial data input.\n",
    "        generated_sample = Input(shape=self.data_dim, name=\"Generated_data\")\n",
    "        # Label input.\n",
    "        label = Input(shape=(1,), dtype=\"int32\", name=\"Label\") \n",
    "        \n",
    "        # Embedd label as onehot vector.\n",
    "        label_embedding = Flatten(name=\"Flatten\")(Embedding(self.num_classes, self.data_dim, name=\"Embedding\")(label))\n",
    "        \n",
    "        # Multiply fake data sample with label embedding to get critic input.\n",
    "        model_input = multiply([generated_sample, label_embedding], name=\"Multiply\")\n",
    "        \n",
    "        validity = model(model_input)\n",
    "\n",
    "        return Model(inputs=[generated_sample, label], \n",
    "                     outputs=validity, \n",
    "                     name=\"Critic\")\n",
    "    \n",
    "    def build_classifier(self):\n",
    "        \n",
    "        model = Sequential(name=\"Classifier\")\n",
    "        \n",
    "        # First hidden layer.\n",
    "        model.add(Dense(128, input_dim=self.data_dim))\n",
    "        model.add(ReLU())\n",
    "        model.add(Dropout(0.3))\n",
    "        \n",
    "        # Second hidden layer.\n",
    "        model.add(Dense(256))\n",
    "        model.add(ReLU())\n",
    "        model.add(Dropout(0.3))\n",
    "        \n",
    "        model.add(Dense(128))\n",
    "        model.add(ReLU())\n",
    "        model.add(Dropout(0.3))\n",
    "        \n",
    "        # Output layer.\n",
    "        model.add(Dense(self.num_classes))\n",
    "        model.add(Softmax())\n",
    "        \n",
    "        model.summary()\n",
    "        \n",
    "        # Data input.\n",
    "        data = Input(shape=self.data_dim, name=\"Data\")\n",
    "\n",
    "        # CLassifier outout is class predictions vector.\n",
    "        predictions = model(data)\n",
    "        \n",
    "        return Model(inputs=data,\n",
    "                     outputs=predictions,\n",
    "                     name=\"Classifier\")\n",
    "        \n",
    "        \n",
    "    def train(self, epochs):\n",
    "        \n",
    "        self.epochs = epochs\n",
    "\n",
    "        # Adversarial ground truths.\n",
    "        valid = -(np.ones((self.batch_size, 1)))\n",
    "        fake =  np.ones((self.batch_size, 1))\n",
    "        dummy = np.zeros((self.batch_size, 1))\n",
    "\n",
    "        # Number of batches.\n",
    "        self.n_batches = math.floor(self.x_train.shape[0] / self.batch_size)\n",
    "\n",
    "        overhead = self.x_train.shape[0] % self.batch_size\n",
    "         \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            # Reset training set.\n",
    "            self.x_train = x_train.copy()\n",
    "            self.y_train = y_train.copy()\n",
    "\n",
    "            # Select random overhead rows that do not fit into batches.\n",
    "            rand_overhead_idx = np.random.choice(range(self.x_train.shape[0]), overhead, replace=False)\n",
    "\n",
    "            # Remove random overhead rows.\n",
    "            self.x_train = np.delete(self.x_train, rand_overhead_idx, axis=0)\n",
    "            self.y_train = np.delete(self.y_train, rand_overhead_idx, axis=0)\n",
    "\n",
    "\n",
    "            # Split training data into batches.\n",
    "            x_batches = np.split(self.x_train, self.n_batches)\n",
    "            y_batches = np.split(self.y_train, self.n_batches)\n",
    "            \n",
    "            for x_batch, y_batch, i in zip(x_batches, y_batches, range(self.n_batches)):   \n",
    "                \n",
    "                if epoch < 5:\n",
    "                    \n",
    "                    for _ in range(self.n_critic):\n",
    "\n",
    "                        # ---------------------\n",
    "                        #  Train Critic\n",
    "                        # ---------------------\n",
    "\n",
    "                        # Generate random noise.\n",
    "                        noise = np.random.normal(0, 1, (self.batch_size, self.latent_dim))\n",
    "\n",
    "                        # Train the critic.\n",
    "                        d_loss = self.critic_model.train_on_batch(\n",
    "                            [x_batch, y_batch, noise],                                      \n",
    "                            [valid, fake, dummy])\n",
    "\n",
    "\n",
    "                    # ---------------------\n",
    "                    #  Train Generator\n",
    "                    # ---------------------\n",
    "\n",
    "                    # Generate sample of artificial labels.\n",
    "                    generated_labels = np.random.randint(1, self.num_classes, self.batch_size).reshape(-1, 1)\n",
    "\n",
    "                    # Train generator.\n",
    "                    g_loss = self.generator_model.train_on_batch([noise, generated_labels], valid)\n",
    "\n",
    "\n",
    "                    # ---------------------\n",
    "                    #  Train Classifier\n",
    "                    # ---------------------\n",
    "\n",
    "                    # One-hot encode real labels.\n",
    "                    y_batch = to_categorical(y_batch, self.num_classes)\n",
    "\n",
    "                    # One-hot encode generated labels.\n",
    "                    generated_labels_onehot = to_categorical(generated_labels, self.num_classes)\n",
    "\n",
    "                    real_loss = self.real_classifier_model.train_on_batch(x_batch, y_batch)\n",
    "\n",
    "                    fake_loss = self.fake_classifier_model.train_on_batch([noise, generated_labels], generated_labels_onehot)\n",
    "\n",
    "                    # Classifier loss as presented in EC-GAN paper.\n",
    "                    c_loss = (real_loss[0] + fake_loss[0]) / (1 + self.adv_weight)\n",
    "\n",
    "                    avg_acc = np.mean([real_loss[1], fake_loss[1]])\n",
    "                \n",
    "                else:\n",
    "                    \n",
    "                    # ---------------------\n",
    "                    #  Train Classifier\n",
    "                    # ---------------------\n",
    "                    \n",
    "                    # Generate random noise.\n",
    "                    noise = np.random.normal(0, 1, (self.batch_size, self.latent_dim))\n",
    "\n",
    "                    # Generate sample of artificial labels.\n",
    "                    generated_labels = np.random.randint(1, self.num_classes, self.batch_size).reshape(-1, 1)\n",
    "\n",
    "                    # One-hot encode real labels.\n",
    "                    y_batch = to_categorical(y_batch, self.num_classes)\n",
    "\n",
    "                    # One-hot encode generated labels.\n",
    "                    generated_labels_onehot = to_categorical(generated_labels, self.num_classes)\n",
    "\n",
    "                    real_loss = self.real_classifier_model.train_on_batch(x_batch, y_batch)\n",
    "\n",
    "                    fake_loss = self.fake_classifier_model.train_on_batch([noise, generated_labels], generated_labels_onehot)\n",
    "\n",
    "                    # Classifier loss as presented in EC-GAN paper.\n",
    "                    c_loss = (real_loss[0] + fake_loss[0]) / (1 + self.adv_weight)\n",
    "\n",
    "                    avg_acc = np.mean([real_loss[1], fake_loss[1]])\n",
    "\n",
    "\n",
    "                # ---------------------\n",
    "                #  Logging\n",
    "                # ---------------------\n",
    "\n",
    "                self.losslog.append([d_loss[0], g_loss, c_loss])\n",
    "                self.class_loss_log.append([real_loss[0], fake_loss[0], c_loss])\n",
    "                self.class_acc_log.append([real_loss[1], fake_loss[1], avg_acc])\n",
    "\n",
    "                # Plot progress.\n",
    "                DLOSS = \"%.4f\" % d_loss[0]\n",
    "                GLOSS = \"%.4f\" % g_loss\n",
    "                CLOSS = \"%.4f\" % c_loss\n",
    "                RLOSS = \"%.4f\" % real_loss[0]\n",
    "                FLOSS = \"%.4f\" % fake_loss[0]\n",
    "                CACC  = \"%.4f\" % real_loss[1]\n",
    "                \n",
    "                if i % 100 == 0:\n",
    "                    print (f\"{epoch} - {i}/{self.n_batches} \\t [D loss: {DLOSS}] [G loss: {GLOSS}] [R loss: {RLOSS} | F loss: {FLOSS} | C loss: {CLOSS} - C acc: {CACC}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gan = ECGAN(x_train,\n",
    "            y_train,\n",
    "            num_classes=15,\n",
    "            latent_dim=32,\n",
    "            batch_size=128,\n",
    "            n_critic=5,\n",
    "            conf_thresh=.2,\n",
    "            adv_weight=.1\n",
    "            )\n",
    "\n",
    "gan.train(epochs=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "326.516px",
    "left": "920.969px",
    "right": "20px",
    "top": "81px",
    "width": "734.469px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
